{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Segmentation\n",
    "\n",
    "## Dependancies\n",
    "\n",
    "- Python(>=3.6)\n",
    "- os\n",
    "- urllib.request\n",
    "- tarfile\n",
    "- python-crfsuite\n",
    "- bpe\n",
    "- numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data\n",
    "\n",
    "Start by running the following code. It will create a directory `.data` and download English segmentation data into the directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .data\n",
      "Downloading segmentation_data.tgz into .data\n",
      "Extracting .data/segmentation_data.tgz\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request, tarfile\n",
    "\n",
    "URL = \"http://mpsilfve.github.io/assets/segmentation_data.tgz\"\n",
    "DATADIR = \".data\"\n",
    "FN = \"segmentation_data\"\n",
    "TGZ = os.path.join(DATADIR,\"%s.tgz\" % FN)\n",
    "\n",
    "try:\n",
    "    print(\"Creating\",DATADIR)\n",
    "    os.mkdir(DATADIR)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "print(\"Downloading %s.tgz into .data\" % FN)\n",
    "urllib.request.urlretrieve(URL,TGZ)\n",
    "\n",
    "print(\"Extracting %s\" % TGZ)\n",
    "tf = tarfile.open(TGZ)\n",
    "tf.extractall(DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Reading data files into lists\n",
    "\n",
    "In the sub-directory `segmentation_data` of the directory `.data`, there are three files: `traindata`, `devdata` and `testdata`. These files consists of two tab-separated columns:\n",
    "\n",
    "```\n",
    "vouchsafed      vouch saf ed\n",
    "negative        negat ive\n",
    "annotations     an not at ion s\n",
    "torpedos        torpedo s\n",
    "coxswain        coxswain\n",
    "lofted          loft ed\n",
    "...\n",
    "```\n",
    "\n",
    "The first column contains a word form and the second column contains the same word form but segmented into morphemes. Please read these files into three lists: `traindata`, `devdata` and `testdata`. Each list element should be a pair like:\n",
    "\n",
    "```\n",
    "[\"vouchsafed\",[\"vouch\",\"saf\",\"ed\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = []\n",
    "devdata = []\n",
    "testdata = []\n",
    "\n",
    "directory = \".data/segmentation_data/\"\n",
    "file_names = [\"traindata\", \"devdata\", \"testdata\"]\n",
    "files = [traindata, devdata, testdata]\n",
    "\n",
    "for i, file in enumerate(file_names):\n",
    "    f = open(directory+file)\n",
    "    for line in f:\n",
    "        line = line.strip(\"\\n\")\n",
    "        word, morphemes = line.split(\"\\t\")\n",
    "        files[i].append([word, morphemes.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unsegmented word forms\n",
    "\n",
    "trainwords = [wf for wf, _ in traindata]\n",
    "devwords = [wf for wf, _ in devdata]\n",
    "testwords = [wf for wf, _ in testdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIES notation\n",
    "\n",
    "Convert segmented word forms into so called BIES (Begin-Inside-End-Singleton) format which looks like this (for the word form \"bunkhouses\" segmented as \"bunk\" \"house\" \"s\"):\n",
    "\n",
    "```\n",
    "BEGIN   INSIDE   INSIDE   END   BEGIN   INSIDE   INSIDE   INSIDE   END   SINGLE\n",
    "\n",
    "b       u        n        k     h       o        u        s        e     s\n",
    "```\n",
    "\n",
    "The first character of each morpheme receives a `BEGIN` tag, the last one an `END` tag and the remaining characters receive an `INSIDE` tag. As a special case, morphemes consisting of a single character (like the plural \"s\" ending above), receive the tag `SINGLE`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN = \"BEGIN\"\n",
    "INSIDE = \"INSIDE\"\n",
    "END = \"END\" \n",
    "SINGLE = \"SINGLE\"\n",
    "\n",
    "def get_bies_notation(data):\n",
    "    '''\n",
    "    Turn the given word and morphmes into bies notation\n",
    "    '''\n",
    "    notations = []\n",
    "    for line in data:\n",
    "        mor_notation = []\n",
    "        morphemes = line[1]\n",
    "        for morpheme in morphemes:\n",
    "            if len(morpheme) == 1:\n",
    "                mor_notation.append([morpheme, SINGLE])\n",
    "            else:\n",
    "                mor_notation.append([morpheme[0], BEGIN])\n",
    "                for i in range(1, len(morpheme) - 1):\n",
    "                    mor_notation.append([morpheme[i], INSIDE])\n",
    "                mor_notation.append([morpheme[-1], END])\n",
    "                \n",
    "        notations.append(mor_notation)\n",
    "\n",
    "    return notations\n",
    "\n",
    "trainbies = get_bies_notation(traindata)\n",
    "devbies = get_bies_notation(devdata)\n",
    "testbies = get_bies_notation(testdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From BIES notation to morphemes\n",
    "\n",
    "For evaluation purposes, we will need to transform BIES notation produced by our segmentation model back into segmented word forms, i.e. take the following as input:\n",
    "\n",
    "```\n",
    "[['s', 'BEGIN'], ['a', 'INSIDE'], ['l', 'INSIDE'], ['e', 'END'], ['s', 'SINGLE']]\n",
    "```\n",
    "\n",
    "And generate the following as output:\n",
    "\n",
    "```\n",
    "[\"sale\", \"s\"]\n",
    "```\n",
    "\n",
    "Implement a function `unbies(data)` that takes a list of examples in BIES format as input and returns a list of pairs in the format:\n",
    "\n",
    "```\n",
    "[\"sales\", [\"sale\",\"s\"]]\n",
    "```\n",
    "\n",
    "The first element in the pair is the unsegmented word form and the second one is the segmented word form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbies(bies_notations):\n",
    "    '''\n",
    "    Get the word form and morphemes from the BIES notations\n",
    "    '''\n",
    "    unbies_forms = []\n",
    "    \n",
    "    for bies_notation in bies_notations:\n",
    "        chars = []\n",
    "        morphemes = []\n",
    "        morpheme = []\n",
    "        for char in bies_notation:\n",
    "            chars.append(char[0])\n",
    "            if char[1] == 'BEGIN' or char[1] == 'INSIDE':\n",
    "                morpheme.append(char[0])\n",
    "            elif char[1] == 'END' or char[1] == 'SINGLE':\n",
    "                morpheme.append(char[0])\n",
    "                morphemes.append(\"\".join(morpheme))\n",
    "                morpheme = []\n",
    "        \n",
    "        unbies_forms.append([\"\".join(chars), morphemes])\n",
    "        \n",
    "    return unbies_forms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a BPE model\n",
    "\n",
    "Initialize a `bpe.Encoder` model `encoder` with vocabulary size 64,000, then read training data for BPE from the file `en-ud-train.txt` in the sub-directory `segmentation_data` in the `.data` directory. Split the file into lines and call the `encoder.fit` giving the dataset as parameter.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bpe.encoder import Encoder\n",
    "\n",
    "VOCAB_SIZE=64000\n",
    "PCTBPE=1\n",
    "NGRAM_MAX=1000000\n",
    "NGRAM_MIN=1\n",
    "\n",
    "encoder = Encoder(VOCAB_SIZE, PCTBPE, ngram_max = NGRAM_MAX, ngram_min = NGRAM_MIN)\n",
    "\n",
    "txt_dir = \"./.data/segmentation_data/en-ud-train.txt\"\n",
    "\n",
    "f = open(txt_dir)\n",
    "corpus = f.read()\n",
    "encoder.fit(corpus.split('\\n'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmenting development and test data using BPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['sales'],\n",
       "  ['oppress', 'or'],\n",
       "  ['wi', 'pes'],\n",
       "  ['bash', 'fully'],\n",
       "  ['feli', 'cit', 'ous'],\n",
       "  ['cere', 'brum'],\n",
       "  ['cents', \"'\"],\n",
       "  ['string', 'ing'],\n",
       "  ['rever', 'tible'],\n",
       "  ['cere', 'al', \"'\", 's'],\n",
       "  ['diso', 'rient', 'ation'],\n",
       "  ['muz', 'zl', 'ed'],\n",
       "  ['chocolate', 's'],\n",
       "  ['alter'],\n",
       "  ['man', '-', 'eating'],\n",
       "  ['vac', 'illa', 'tion'],\n",
       "  ['je', 'll', '-', 'o', \"'\", 's'],\n",
       "  ['ill', 'ness', \"'\"],\n",
       "  ['methods'],\n",
       "  ['tres', 'pass'],\n",
       "  ['with', 'hold'],\n",
       "  ['years', \"'\"],\n",
       "  ['mitigate'],\n",
       "  ['rail', 'way', \"'\", 's'],\n",
       "  ['undertake', 'r', \"'\", 's'],\n",
       "  ['hot', 'house'],\n",
       "  ['slo', 'tted'],\n",
       "  ['stra', 'fed'],\n",
       "  ['blu', 'ster', \"'\", 's'],\n",
       "  ['dinner', '-', 'service'],\n",
       "  ['off', '-', 'white', \"'\", 's'],\n",
       "  ['right'],\n",
       "  ['returns', \"'\"],\n",
       "  ['tribu', 'nal', \"'\", 's'],\n",
       "  ['chron', 'ically'],\n",
       "  ['protest', 'ants'],\n",
       "  ['professors', 'hips'],\n",
       "  ['jun', 'gles'],\n",
       "  ['miz', 'zen'],\n",
       "  ['chlorin', 'e'],\n",
       "  ['sand', 'alw', 'ood'],\n",
       "  ['sub', 'urban', 'ite'],\n",
       "  ['win', 'ced'],\n",
       "  ['freighte', 'd'],\n",
       "  ['punch'],\n",
       "  ['algeria'],\n",
       "  ['pup', 'a'],\n",
       "  ['describ', 'able'],\n",
       "  ['bowe', 'ls', \"'\"],\n",
       "  ['brav', 'ura'],\n",
       "  ['nick', 'ed'],\n",
       "  ['comput', 'ations'],\n",
       "  ['prevented'],\n",
       "  ['tie', '-', 'on'],\n",
       "  ['prece', 'pts'],\n",
       "  ['harbour', 's'],\n",
       "  ['sweet', \"'\", 's'],\n",
       "  ['badly', '-', 'off'],\n",
       "  ['sno', 'b', \"'\", 's'],\n",
       "  ['puts'],\n",
       "  ['fau', 'na'],\n",
       "  ['attitude', \"'\", 's'],\n",
       "  ['gou', 'rman', 'ds'],\n",
       "  ['arrangements'],\n",
       "  ['school', 'mistr', 'ess', \"'\"],\n",
       "  ['mind', '-', 'reader'],\n",
       "  ['interp', 'osing'],\n",
       "  ['matr', 'iar', 'chal'],\n",
       "  ['bet', 'ray', 'al'],\n",
       "  ['reins', \"'\"],\n",
       "  ['quiet', 'er'],\n",
       "  ['foot', 'falls'],\n",
       "  ['am', 'hara'],\n",
       "  ['money', '-', 'gru', 'bber'],\n",
       "  ['ture', 'en'],\n",
       "  ['dru', 'mh', 'eads'],\n",
       "  ['unse', 'em', 'ly'],\n",
       "  ['beauti', 'cians', \"'\"],\n",
       "  ['entrea', 'ting'],\n",
       "  ['looking', '-', 'glasses'],\n",
       "  ['game', 'cock', 's'],\n",
       "  ['reek', \"'\", 's'],\n",
       "  ['potte', 'r'],\n",
       "  ['standard', 'izes'],\n",
       "  ['foresee', 's'],\n",
       "  ['motor', 'bike', 's'],\n",
       "  ['summari', 'es'],\n",
       "  ['hitch', 'hik', 'er', \"'\", 's'],\n",
       "  ['hi', '-', 'fi'],\n",
       "  ['dand', 'led'],\n",
       "  ['prophe', 'sy'],\n",
       "  ['stal', 'wart'],\n",
       "  ['stage', 'coa', 'ch', \"'\", 's'],\n",
       "  ['unders', 'core'],\n",
       "  ['gaw', 'ping'],\n",
       "  ['lust', 'iness'],\n",
       "  ['measur', 'ably'],\n",
       "  ['sale', 'room'],\n",
       "  ['vast'],\n",
       "  ['photo', \"'\", 's'],\n",
       "  ['glow', 's'],\n",
       "  ['mur', 'als', \"'\"],\n",
       "  ['purport'],\n",
       "  ['peel', \"'\", 's'],\n",
       "  ['earth', 'bound'],\n",
       "  ['coa', 'ch'],\n",
       "  ['tele', 'graph'],\n",
       "  ['bert', 'hing'],\n",
       "  ['recommendations', \"'\"],\n",
       "  ['tart', 'ars', \"'\"],\n",
       "  ['luck', 'less'],\n",
       "  ['too', 'tha', 'ches'],\n",
       "  ['heart', 'burn'],\n",
       "  ['romantic', \"'\", 's'],\n",
       "  ['caps', 'icu', 'ms'],\n",
       "  ['una', 'dopted'],\n",
       "  ['motor', 'ize'],\n",
       "  ['clic', 'hed'],\n",
       "  ['pi', 'ano', 'las'],\n",
       "  ['medit', 'erra', 'nea', 'n'],\n",
       "  ['reviv', 'alist'],\n",
       "  ['pro', 'rogat', 'ion'],\n",
       "  ['subsequently'],\n",
       "  ['remo', 'rse', 'fully'],\n",
       "  ['store', 'rooms'],\n",
       "  ['sheila', 's'],\n",
       "  ['lur', 'idne', 'ss'],\n",
       "  ['sho', 'gun'],\n",
       "  ['smoo', 'ched'],\n",
       "  ['pastor', 'als'],\n",
       "  ['penetrate', 's'],\n",
       "  ['refr', 'acts'],\n",
       "  ['hap', \"'\", 's'],\n",
       "  ['poo', 'fs'],\n",
       "  ['masc', 'ot'],\n",
       "  ['bit'],\n",
       "  ['rockin', 'ess'],\n",
       "  ['mathematic', 'ian', \"'\", 's'],\n",
       "  ['switch', 'ing'],\n",
       "  ['cit', 'ron', \"'\", 's'],\n",
       "  ['regre', 'ssive'],\n",
       "  ['reserved'],\n",
       "  ['hurricane', 's', \"'\"],\n",
       "  ['blin', 'kers'],\n",
       "  ['rail', '-', 'car'],\n",
       "  ['sha', 'ys', \"'\"],\n",
       "  ['stock', 'ing'],\n",
       "  ['kids'],\n",
       "  ['overl', 'apped'],\n",
       "  ['intimat', 'ing'],\n",
       "  ['worse'],\n",
       "  ['interc', 'hanged'],\n",
       "  ['romantic', 'ists'],\n",
       "  ['unpro', 'fessionally'],\n",
       "  ['fanc', 'iers', \"'\"],\n",
       "  ['bane'],\n",
       "  ['nav', 'ies', \"'\"],\n",
       "  ['joke', 'r'],\n",
       "  ['steam', 'ship'],\n",
       "  ['tempte', 'r'],\n",
       "  ['sno', 'bs', \"'\"],\n",
       "  ['plu', 'cky'],\n",
       "  ['emb', 'ossing'],\n",
       "  ['tre', 'llis', 'ing'],\n",
       "  ['croco', 'dile', \"'\", 's'],\n",
       "  ['tut', 'ting'],\n",
       "  ['corner', 'ed'],\n",
       "  ['dress', 'ings', \"'\"]],\n",
       " [['bunk', 'houses'],\n",
       "  ['beh', 'eld'],\n",
       "  ['hardest', '-', 'heart', 'ed'],\n",
       "  ['kness', 'et'],\n",
       "  ['bac', 'cala', 'urea', 'te'],\n",
       "  ['carri', 'ages'],\n",
       "  ['blind', 'er', \"'\", 's'],\n",
       "  ['dissol', 'ute'],\n",
       "  ['hal', 'oes'],\n",
       "  ['trum', 'peters'],\n",
       "  ['sun', 'roof'],\n",
       "  ['lou', 'vr', 'ed'],\n",
       "  ['meeting', 'house'],\n",
       "  ['prod', 'ded'],\n",
       "  ['seque', 'l', \"'\", 's'],\n",
       "  ['analyst'],\n",
       "  ['fist', 'ula', 's'],\n",
       "  ['pap', 'rik', 'a'],\n",
       "  ['stru', 'ng'],\n",
       "  ['ill', 'ness', \"'\", 's'],\n",
       "  ['flu', 'rried'],\n",
       "  ['gross', \"'\"],\n",
       "  ['cri', 'ers'],\n",
       "  ['further', 'ing'],\n",
       "  ['cover'],\n",
       "  ['substan', 'dard'],\n",
       "  ['buck', 'les', \"'\"],\n",
       "  ['park'],\n",
       "  ['rein', \"'\", 's'],\n",
       "  ['cogn', 'ate'],\n",
       "  ['bound', 'aries'],\n",
       "  ['embe', 'llis', 'hing'],\n",
       "  ['launche', 's', \"'\"],\n",
       "  ['move', 'rs', \"'\"],\n",
       "  ['regulations'],\n",
       "  ['rocke', 'r', \"'\", 's'],\n",
       "  ['training'],\n",
       "  ['business', \"'\"],\n",
       "  ['manda', 'te', \"'\", 's'],\n",
       "  ['freak', 'ing'],\n",
       "  ['nin', 'nies'],\n",
       "  ['lur', 'chers'],\n",
       "  ['vas', 'sals'],\n",
       "  ['bri', 'ber', 'y'],\n",
       "  ['gang', 'ers'],\n",
       "  ['hal', 't'],\n",
       "  ['benef', 'actor', \"'\", 's'],\n",
       "  ['se'],\n",
       "  ['placement'],\n",
       "  ['assets'],\n",
       "  ['wash', 'stands'],\n",
       "  ['extensive', 'ness'],\n",
       "  ['wilt', 'ons'],\n",
       "  ['reduce'],\n",
       "  ['elder'],\n",
       "  ['cult', 's'],\n",
       "  ['shen', 'ani', 'gans'],\n",
       "  ['trivial', 'ized'],\n",
       "  ['made', 'ira', \"'\", 's'],\n",
       "  ['sque', 'akers'],\n",
       "  ['intense', 'ly'],\n",
       "  ['demo', 'ralize'],\n",
       "  ['hoa', 'rfr', 'ost'],\n",
       "  ['tot', 'ing'],\n",
       "  ['squi', 'nts'],\n",
       "  ['middle', '-', 'distance'],\n",
       "  ['coinciden', 'ces'],\n",
       "  ['twin', \"'\", 's'],\n",
       "  ['power', 'ed'],\n",
       "  ['gib', 'ral', 'tarian'],\n",
       "  ['toward'],\n",
       "  ['overl', 'apping'],\n",
       "  ['jo', 'ggling'],\n",
       "  ['sal', 'vo', 'es'],\n",
       "  ['mono', 'gamous'],\n",
       "  ['juven', 'iles'],\n",
       "  ['shop', 'lift', 's'],\n",
       "  ['bear', 'er'],\n",
       "  ['washe', 'rwo', 'man'],\n",
       "  ['cauc', 'uses'],\n",
       "  ['sinking', '-', 'fund'],\n",
       "  ['marsh', \"'\", 's'],\n",
       "  ['tests'],\n",
       "  ['layers', \"'\"],\n",
       "  ['rational', 'ist'],\n",
       "  ['unse', 'em', 'liness'],\n",
       "  ['fi', 'zz', 'led'],\n",
       "  ['influential', 'ly'],\n",
       "  ['spread'],\n",
       "  ['astron', 'omers', \"'\"],\n",
       "  ['puni', 'c'],\n",
       "  ['joine', 'ry'],\n",
       "  ['diso', 'rders'],\n",
       "  ['gai', 'ter', \"'\", 's'],\n",
       "  ['wel', 'shing'],\n",
       "  ['under'],\n",
       "  ['peri'],\n",
       "  ['sea', '-', 'lion'],\n",
       "  ['flat', '-', 'foot', 'ed'],\n",
       "  ['white', 'caps'],\n",
       "  ['talking', '-', 'points'],\n",
       "  ['having'],\n",
       "  ['pur', 'itani', 'sm'],\n",
       "  ['gaw', 'p'],\n",
       "  ['gli', 'des'],\n",
       "  ['bla', 'bber'],\n",
       "  ['mane'],\n",
       "  ['ser', 'pent', 's', \"'\"],\n",
       "  ['turn', 'coat', 's'],\n",
       "  ['put', 'ref', 'y'],\n",
       "  ['skimme', 'd', '-', 'milk'],\n",
       "  ['curat', 'orship', 's'],\n",
       "  ['num', 'skull', 's'],\n",
       "  ['describe'],\n",
       "  ['schedule', 's'],\n",
       "  ['matter', \"'\", 's'],\n",
       "  ['world', '-', 'wear', 'y'],\n",
       "  ['tortur', 'ers'],\n",
       "  ['training', \"'\", 's'],\n",
       "  ['rem', 'unera', 'ted'],\n",
       "  ['warl', 'ike'],\n",
       "  ['congress', \"'\", 's'],\n",
       "  ['phae', 'tons'],\n",
       "  ['exte', 'mpora', 'neously'],\n",
       "  ['der', 'v'],\n",
       "  ['dream', 'ier'],\n",
       "  ['habit', 'ues'],\n",
       "  ['counter', 'fei', 'ting'],\n",
       "  ['unab', 'ridge', 'd'],\n",
       "  ['skul', 'ked'],\n",
       "  ['onet', 'ime'],\n",
       "  ['arbitra', 'rines', 's', \"'\"],\n",
       "  ['contin', 'gency'],\n",
       "  ['station', 'er'],\n",
       "  ['ex', '-', 'service', 'man'],\n",
       "  ['undo', 'ne'],\n",
       "  ['accent', 's', \"'\"],\n",
       "  ['discovery', \"'\", 's'],\n",
       "  ['heavi', 'es'],\n",
       "  ['symboli', 'cally'],\n",
       "  ['transi', 'ent'],\n",
       "  ['insula', 'tion', \"'\", 's'],\n",
       "  ['test', '-', 'drive', 's'],\n",
       "  ['gel', 'dings'],\n",
       "  ['cho', 'ked'],\n",
       "  ['ido', 'lizing'],\n",
       "  ['fresh', 'er'],\n",
       "  ['false', 'hoods', \"'\"],\n",
       "  ['slice', 's'],\n",
       "  ['wake', 'n'],\n",
       "  ['sinf', 'ully'],\n",
       "  ['challenge', 'r', \"'\", 's'],\n",
       "  ['film', 's'],\n",
       "  ['toe', '-', 'cap'],\n",
       "  ['gib', 'bon'],\n",
       "  ['wood', \"'\", 's'],\n",
       "  ['walk', '-', 'up'],\n",
       "  ['spec', 'ulated'],\n",
       "  ['crook', 'edness'],\n",
       "  ['foot', '-', 'fault', 'ing'],\n",
       "  ['bu', 'ffe', 'ts'],\n",
       "  ['tria', 'ds'],\n",
       "  ['smooth', 'er'],\n",
       "  ['face', 't', \"'\", 's'],\n",
       "  ['placed'],\n",
       "  ['tack', 'les'],\n",
       "  ['preci', 'pita', 'tes'],\n",
       "  ['salt', \"'\", 's']]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_segmented_dev = []\n",
    "bpe_segmented_test = []\n",
    "\n",
    "todo_words = [devwords, testwords]\n",
    "bpe_segmented = [bpe_segmented_dev, bpe_segmented_test]\n",
    "\n",
    "for i, words in enumerate(todo_words):\n",
    "    for word in words:\n",
    "        bpe_segmented[i].append([segment for segment in encoder.tokenize(word) if not segment.startswith(\"__\")])\n",
    "        \n",
    "bpe_segmented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We will evaluate segmentation algorithms using precision, recall and f-score on segment boundaries. As an example, let's say we evaluate against the following gold standard:\n",
    "\n",
    "```\n",
    "[[\"hot\", \"dog\",\"s\"],[\"king\",\"s\"]]\n",
    "```\n",
    "\n",
    "and our segmentation system returned the following segmentation:\n",
    "\n",
    "```\n",
    "[[\"hot\",\"dog\",\"s\"], [\"k\",\"ing\",\"s\"]]\n",
    "```\n",
    "\n",
    "The gold standard segment boundaries are:\n",
    "\n",
    "```\n",
    "[[3,6],[4]]\n",
    "```\n",
    "\n",
    "and our system gives the following segment boundaries:\n",
    "\n",
    "```\n",
    "[[3,6],[1,4]]\n",
    "```\n",
    "\n",
    "Now three of our segment boundaries are actually found in the gold standard (`3` and `6` for the first example and `4` for the second one). This gives us a precision of $P = 3/4 = 75\\%$ (three of the four morpheme boundaries were found are in the gold standard) and a recall of $R = 3/3 = 100\\%$ (we found two of the three token boundaries given by the gold standard). Finally, we get the f-score as $2 \\cdot P \\cdot R /(P+R) \\approx 85.7\\%$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BPE segmentation:\n",
      "Test set precision: 0.46, recall: 0.48, f-score: 0.47\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_boundaries(segmented_data):\n",
    "    '''\n",
    "    Return the morpheme boundaries given the segmented morphemes\n",
    "    '''\n",
    "    boundaries = []\n",
    "    len_segments = [len(seg) for seg in segmented_data]\n",
    "\n",
    "    if len(len_segments) > 1:\n",
    "        boundaries.append(len_segments[0])\n",
    "        for i in range(1, len(len_segments) - 1):\n",
    "            boundaries.append(len_segments[i] + boundaries[i - 1])\n",
    "        \n",
    "    return boundaries  \n",
    "        \n",
    "\n",
    "def evaluate(sys_segmented_data,gold_segmented_data):\n",
    "\n",
    "    correct = 0\n",
    "    sys_total = 0\n",
    "    gold_total = 0\n",
    "    \n",
    "    for i in range(len(sys_segmented_data)):\n",
    "        sys_boundary = get_boundaries(sys_segmented_data[i])\n",
    "        gold_boundary = get_boundaries(gold_segmented_data[i][1])\n",
    "        sys_total += len(sys_boundary)\n",
    "        gold_total += len(gold_boundary)\n",
    "        for b in sys_boundary:\n",
    "            if b in gold_boundary:\n",
    "                correct += 1\n",
    "    \n",
    "    precision = correct/sys_total\n",
    "    recall = correct/gold_total\n",
    "    fscore = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, fscore\n",
    "    \n",
    "precision, recall, fscore = evaluate(bpe_segmented_test,testdata)\n",
    "print(\"Results for BPE segmentation:\")\n",
    "print(\"Test set precision: %.2f, recall: %.2f, f-score: %.2f\" % (precision, recall, fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised morphological segmentation\n",
    "\n",
    "Install `pycrfsuite` using pip (see [Installation](https://python-crfsuite.readthedocs.io/en/latest/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "\n",
    "Implement a feature extraction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOUNDARY=\"<BD>\"\n",
    "\n",
    "def char2features(example, i):\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    char_at_i = example[i][0]\n",
    "    distance_to_end = str(len(example) - i)\n",
    "    isalpha_at_i = str(example[i][0].isalpha())\n",
    "    isdigit_at_i = str(example[i][0].isdigit())\n",
    "    \n",
    "    padded_example = [BOUNDARY, BOUNDARY] + [e[0] for e in example] + [BOUNDARY, BOUNDARY]\n",
    "    \n",
    "    left_char2 = padded_example[i]\n",
    "    left_char = padded_example[i + 1]\n",
    "    right_char = padded_example[i + 3]\n",
    "    right_char2 = padded_example[i + 4]\n",
    "    \n",
    "    features.append(\"CHARACTER_AT_i=%s\" % char_at_i)\n",
    "    \n",
    "    features.append(\"LEFT_CHARACTER=%s\" % left_char)\n",
    "    features.append(\"RIGHT_CHARACTER=%s\" % right_char)\n",
    "    \n",
    "    features.append(\"LEFT_BIGRAM=%s\" % left_char+char_at_i)\n",
    "    features.append(\"RIGHT_BIGRAM=%s\" % char_at_i+right_char)\n",
    "    \n",
    "    features.append(\"LEFT_SKIPGRAM=%s\" % left_char2+char_at_i)\n",
    "    features.append(\"RIGHT_SKIPGRAM=%s\" % char_at_i+right_char2)\n",
    "    \n",
    "    features.append(\"LEFT_TRIGRAM=%s\" % left_char2+left_char+char_at_i)\n",
    "    features.append(\"RIGHT_TRIGRAM=%s\" % char_at_i+right_char+right_char2)\n",
    "    \n",
    "    features.append(\"DISTANCE_TO_END=%s\" % distance_to_end)\n",
    "    \n",
    "    features.append(\"IS_ALPHA=%s\" % isalpha_at_i)\n",
    "    features.append(\"IS_DIGIT=%s\" % isdigit_at_i)\n",
    "\n",
    "    return features\n",
    "\n",
    "def data2features(data):\n",
    "    \"\"\" Extract features for a data set in BIES format. \"\"\"\n",
    "    return [[char2features(example,i) for i in range(len(example))] for example in data]\n",
    "\n",
    "def data2labels(data):\n",
    "    \"\"\" Extract the tags from a data set in BIES format. \"\"\"\n",
    "    return [[tok[1] for tok in example] for example in data]\n",
    "\n",
    "# Initialize the training, development and test sets for pycrfsuite.\n",
    "X_train = data2features(trainbies)\n",
    "y_train = data2labels(trainbies)\n",
    "\n",
    "X_dev = data2features(devbies)\n",
    "y_dev = data2labels(devbies)\n",
    "\n",
    "X_test = data2features(testbies)\n",
    "y_test = data2labels(testbies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CHARACTER_AT_i=s',\n",
       "  'LEFT_CHARACTER=<BD>',\n",
       "  'RIGHT_CHARACTER=a',\n",
       "  'LEFT_BIGRAM=<BD>s',\n",
       "  'RIGHT_BIGRAM=sa',\n",
       "  'LEFT_SKIPGRAM=<BD>s',\n",
       "  'RIGHT_SKIPGRAM=sl',\n",
       "  'LEFT_TRIGRAM=<BD><BD>s',\n",
       "  'RIGHT_TRIGRAM=sal',\n",
       "  'DISTANCE_TO_END=5',\n",
       "  'IS_ALPHA=True',\n",
       "  'IS_DIGIT=False'],\n",
       " ['CHARACTER_AT_i=a',\n",
       "  'LEFT_CHARACTER=s',\n",
       "  'RIGHT_CHARACTER=l',\n",
       "  'LEFT_BIGRAM=sa',\n",
       "  'RIGHT_BIGRAM=al',\n",
       "  'LEFT_SKIPGRAM=<BD>a',\n",
       "  'RIGHT_SKIPGRAM=ae',\n",
       "  'LEFT_TRIGRAM=<BD>sa',\n",
       "  'RIGHT_TRIGRAM=ale',\n",
       "  'DISTANCE_TO_END=4',\n",
       "  'IS_ALPHA=True',\n",
       "  'IS_DIGIT=False'],\n",
       " ['CHARACTER_AT_i=l',\n",
       "  'LEFT_CHARACTER=a',\n",
       "  'RIGHT_CHARACTER=e',\n",
       "  'LEFT_BIGRAM=al',\n",
       "  'RIGHT_BIGRAM=le',\n",
       "  'LEFT_SKIPGRAM=sl',\n",
       "  'RIGHT_SKIPGRAM=ls',\n",
       "  'LEFT_TRIGRAM=sal',\n",
       "  'RIGHT_TRIGRAM=les',\n",
       "  'DISTANCE_TO_END=3',\n",
       "  'IS_ALPHA=True',\n",
       "  'IS_DIGIT=False'],\n",
       " ['CHARACTER_AT_i=e',\n",
       "  'LEFT_CHARACTER=l',\n",
       "  'RIGHT_CHARACTER=s',\n",
       "  'LEFT_BIGRAM=le',\n",
       "  'RIGHT_BIGRAM=es',\n",
       "  'LEFT_SKIPGRAM=ae',\n",
       "  'RIGHT_SKIPGRAM=e<BD>',\n",
       "  'LEFT_TRIGRAM=ale',\n",
       "  'RIGHT_TRIGRAM=es<BD>',\n",
       "  'DISTANCE_TO_END=2',\n",
       "  'IS_ALPHA=True',\n",
       "  'IS_DIGIT=False'],\n",
       " ['CHARACTER_AT_i=s',\n",
       "  'LEFT_CHARACTER=e',\n",
       "  'RIGHT_CHARACTER=<BD>',\n",
       "  'LEFT_BIGRAM=es',\n",
       "  'RIGHT_BIGRAM=s<BD>',\n",
       "  'LEFT_SKIPGRAM=ls',\n",
       "  'RIGHT_SKIPGRAM=s<BD>',\n",
       "  'LEFT_TRIGRAM=les',\n",
       "  'RIGHT_TRIGRAM=s<BD><BD>',\n",
       "  'DISTANCE_TO_END=1',\n",
       "  'IS_ALPHA=True',\n",
       "  'IS_DIGIT=False']]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the segmentation system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 11596\n",
      "Seconds required: 0.056\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 1.000000\n",
      "c2: 0.001000\n",
      "num_memories: 6\n",
      "max_iterations: 50\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "***** Iteration #1 *****\n",
      "Loss: 11922.221199\n",
      "Feature norm: 1.000000\n",
      "Error norm: 4063.747325\n",
      "Active features: 6748\n",
      "Line search trials: 1\n",
      "Line search step: 0.000132\n",
      "Seconds required for this iteration: 0.010\n",
      "\n",
      "***** Iteration #2 *****\n",
      "Loss: 10565.671702\n",
      "Feature norm: 1.237657\n",
      "Error norm: 3058.402218\n",
      "Active features: 7005\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #3 *****\n",
      "Loss: 6386.033361\n",
      "Feature norm: 3.681648\n",
      "Error norm: 2564.218418\n",
      "Active features: 6103\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #4 *****\n",
      "Loss: 4787.198161\n",
      "Feature norm: 4.668145\n",
      "Error norm: 1514.462514\n",
      "Active features: 6137\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #5 *****\n",
      "Loss: 3810.644172\n",
      "Feature norm: 6.294906\n",
      "Error norm: 1665.677683\n",
      "Active features: 6174\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #6 *****\n",
      "Loss: 3330.949990\n",
      "Feature norm: 7.099138\n",
      "Error norm: 576.481511\n",
      "Active features: 6157\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #7 *****\n",
      "Loss: 2947.298965\n",
      "Feature norm: 8.423041\n",
      "Error norm: 734.916830\n",
      "Active features: 6011\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #8 *****\n",
      "Loss: 2565.686040\n",
      "Feature norm: 10.399371\n",
      "Error norm: 551.172902\n",
      "Active features: 5689\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #9 *****\n",
      "Loss: 2340.024614\n",
      "Feature norm: 12.276429\n",
      "Error norm: 348.463368\n",
      "Active features: 5418\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.007\n",
      "\n",
      "***** Iteration #10 *****\n",
      "Loss: 2208.688955\n",
      "Feature norm: 13.738993\n",
      "Error norm: 267.581657\n",
      "Active features: 5103\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.007\n",
      "\n",
      "***** Iteration #11 *****\n",
      "Loss: 2117.041886\n",
      "Feature norm: 15.650758\n",
      "Error norm: 443.781534\n",
      "Active features: 4666\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #12 *****\n",
      "Loss: 2033.416093\n",
      "Feature norm: 16.169500\n",
      "Error norm: 259.189058\n",
      "Active features: 4563\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #13 *****\n",
      "Loss: 1984.754599\n",
      "Feature norm: 16.621958\n",
      "Error norm: 242.496365\n",
      "Active features: 4409\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #14 *****\n",
      "Loss: 1927.124136\n",
      "Feature norm: 17.779075\n",
      "Error norm: 538.868343\n",
      "Active features: 4173\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #15 *****\n",
      "Loss: 1867.919036\n",
      "Feature norm: 18.412525\n",
      "Error norm: 142.800439\n",
      "Active features: 4050\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #16 *****\n",
      "Loss: 1824.094104\n",
      "Feature norm: 18.814515\n",
      "Error norm: 225.253610\n",
      "Active features: 3909\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #17 *****\n",
      "Loss: 1770.758536\n",
      "Feature norm: 19.289149\n",
      "Error norm: 101.078353\n",
      "Active features: 3746\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.010\n",
      "\n",
      "***** Iteration #18 *****\n",
      "Loss: 1730.550497\n",
      "Feature norm: 19.565420\n",
      "Error norm: 224.583678\n",
      "Active features: 3556\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.007\n",
      "\n",
      "***** Iteration #19 *****\n",
      "Loss: 1696.259086\n",
      "Feature norm: 20.114030\n",
      "Error norm: 93.732034\n",
      "Active features: 3429\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.008\n",
      "\n",
      "***** Iteration #20 *****\n",
      "Loss: 1661.834213\n",
      "Feature norm: 21.113041\n",
      "Error norm: 176.666870\n",
      "Active features: 3195\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.007\n",
      "\n",
      "***** Iteration #21 *****\n",
      "Loss: 1639.581130\n",
      "Feature norm: 21.799625\n",
      "Error norm: 107.994903\n",
      "Active features: 3040\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #22 *****\n",
      "Loss: 1619.178428\n",
      "Feature norm: 22.503504\n",
      "Error norm: 107.883668\n",
      "Active features: 2899\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #23 *****\n",
      "Loss: 1596.517352\n",
      "Feature norm: 23.158021\n",
      "Error norm: 58.137037\n",
      "Active features: 2708\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #24 *****\n",
      "Loss: 1581.863578\n",
      "Feature norm: 23.832023\n",
      "Error norm: 208.593374\n",
      "Active features: 2570\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #25 *****\n",
      "Loss: 1571.145348\n",
      "Feature norm: 24.002496\n",
      "Error norm: 33.918959\n",
      "Active features: 2496\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #26 *****\n",
      "Loss: 1560.529836\n",
      "Feature norm: 24.205028\n",
      "Error norm: 39.326365\n",
      "Active features: 2409\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #27 *****\n",
      "Loss: 1552.458989\n",
      "Feature norm: 24.893917\n",
      "Error norm: 198.170710\n",
      "Active features: 2197\n",
      "Line search trials: 2\n",
      "Line search step: 0.500000\n",
      "Seconds required for this iteration: 0.009\n",
      "\n",
      "***** Iteration #28 *****\n",
      "Loss: 1543.747848\n",
      "Feature norm: 25.451573\n",
      "Error norm: 185.137207\n",
      "Active features: 2143\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.006\n",
      "\n",
      "***** Iteration #29 *****\n",
      "Loss: 1538.124223\n",
      "Feature norm: 25.667926\n",
      "Error norm: 32.438743\n",
      "Active features: 2128\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #30 *****\n",
      "Loss: 1533.350716\n",
      "Feature norm: 26.037649\n",
      "Error norm: 130.714623\n",
      "Active features: 2047\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #31 *****\n",
      "Loss: 1527.601135\n",
      "Feature norm: 26.304993\n",
      "Error norm: 42.873683\n",
      "Active features: 1963\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #32 *****\n",
      "Loss: 1524.365328\n",
      "Feature norm: 26.830459\n",
      "Error norm: 186.599870\n",
      "Active features: 1863\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #33 *****\n",
      "Loss: 1519.433140\n",
      "Feature norm: 27.024058\n",
      "Error norm: 26.214398\n",
      "Active features: 1824\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #34 *****\n",
      "Loss: 1516.026430\n",
      "Feature norm: 27.234887\n",
      "Error norm: 22.903462\n",
      "Active features: 1781\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #35 *****\n",
      "Loss: 1512.524847\n",
      "Feature norm: 27.801718\n",
      "Error norm: 121.081097\n",
      "Active features: 1635\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #36 *****\n",
      "Loss: 1509.191852\n",
      "Feature norm: 27.914611\n",
      "Error norm: 75.347672\n",
      "Active features: 1649\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #37 *****\n",
      "Loss: 1507.689926\n",
      "Feature norm: 27.975374\n",
      "Error norm: 20.485490\n",
      "Active features: 1644\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #38 *****\n",
      "Loss: 1505.428731\n",
      "Feature norm: 28.071269\n",
      "Error norm: 28.518803\n",
      "Active features: 1612\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #39 *****\n",
      "Loss: 1504.252796\n",
      "Feature norm: 28.329565\n",
      "Error norm: 154.018563\n",
      "Active features: 1523\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #40 *****\n",
      "Loss: 1500.790735\n",
      "Feature norm: 28.460424\n",
      "Error norm: 20.295952\n",
      "Active features: 1521\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #41 *****\n",
      "Loss: 1499.856046\n",
      "Feature norm: 28.494555\n",
      "Error norm: 17.348075\n",
      "Active features: 1520\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #42 *****\n",
      "Loss: 1496.487830\n",
      "Feature norm: 28.619690\n",
      "Error norm: 40.142676\n",
      "Active features: 1443\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #43 *****\n",
      "Loss: 1495.153487\n",
      "Feature norm: 28.623317\n",
      "Error norm: 82.740015\n",
      "Active features: 1420\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #44 *****\n",
      "Loss: 1493.765395\n",
      "Feature norm: 28.683370\n",
      "Error norm: 54.747804\n",
      "Active features: 1421\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #45 *****\n",
      "Loss: 1492.799958\n",
      "Feature norm: 28.677959\n",
      "Error norm: 71.968661\n",
      "Active features: 1400\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #46 *****\n",
      "Loss: 1491.819320\n",
      "Feature norm: 28.715411\n",
      "Error norm: 59.735638\n",
      "Active features: 1399\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #47 *****\n",
      "Loss: 1490.610667\n",
      "Feature norm: 28.710179\n",
      "Error norm: 43.831700\n",
      "Active features: 1385\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #48 *****\n",
      "Loss: 1489.600753\n",
      "Feature norm: 28.726049\n",
      "Error norm: 64.882020\n",
      "Active features: 1376\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #49 *****\n",
      "Loss: 1488.200496\n",
      "Feature norm: 28.724909\n",
      "Error norm: 57.842503\n",
      "Active features: 1368\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #50 *****\n",
      "Loss: 1487.294137\n",
      "Feature norm: 28.755853\n",
      "Error norm: 92.165070\n",
      "Active features: 1357\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 0.289\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 1357 (11596)\n",
      "Number of active attributes: 902 (7635)\n",
      "Number of active labels: 4 (4)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=True)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train('segmentation.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning and segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for supervised segmentation:\n",
      "Development set precision: 0.89, recall: 0.85, f-score: 0.87\n",
      "Results for supervised segmentation:\n",
      "Test set precision: 0.88, recall: 0.82, f-score: 0.85\n"
     ]
    }
   ],
   "source": [
    "def segment(data,tagger):\n",
    "    tagged_data = []\n",
    "    for i,example in enumerate(data2features(data)):\n",
    "        tags = tagger.tag(example)\n",
    "        tagged_example = [(char, tag) for (char, _), tag in zip(data[i],tags)]\n",
    "        tagged_data.append(tagged_example)\n",
    "    return [segmented for _,segmented in unbies(tagged_data)]\n",
    "    \n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('segmentation.model')\n",
    "\n",
    "supervised_tokenized_dev = segment(devbies,tagger)\n",
    "print(\"Results for supervised segmentation:\")\n",
    "print(\"Development set precision: %.2f, recall: %.2f, f-score: %.2f\" % evaluate(supervised_tokenized_dev,devdata))\n",
    "\n",
    "supervised_tokenized_test = segment(testbies,tagger)\n",
    "print(\"Results for supervised segmentation:\")\n",
    "print(\"Test set precision: %.2f, recall: %.2f, f-score: %.2f\" % evaluate(supervised_tokenized_test,testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
